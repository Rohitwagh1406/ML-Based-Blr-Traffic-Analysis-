{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f81f240",
   "metadata": {},
   "source": [
    "# Traffic Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c8798",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the process of analyzing and predicting traffic patterns in Bangalore using machine learning techniques.\n",
    "\n",
    "## 1. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Banglore_traffic_Dataset 2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Date' column to datetime format and extract time-based features\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Weekday'] = df['Date'].dt.weekday\n",
    "\n",
    "# Encode categorical columns ('Weather Conditions' and 'Roadwork and Construction Activity')\n",
    "label_encoder = LabelEncoder()\n",
    "df['Weather Conditions'] = label_encoder.fit_transform(df['Weather Conditions'])\n",
    "df['Roadwork and Construction Activity'] = label_encoder.fit_transform(df['Roadwork and Construction Activity'])\n",
    "\n",
    "# Drop the original 'Date' column\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4f6b5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizing the distribution of Traffic Volume\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Traffic Volume'], kde=True, bins=30)\n",
    "plt.title('Distribution of Traffic Volume')\n",
    "plt.xlabel('Traffic Volume')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap to identify relationships\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Traffic Volume vs Average Speed\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Average Speed', y='Traffic Volume', data=df)\n",
    "plt.title('Traffic Volume vs Average Speed')\n",
    "plt.xlabel('Average Speed (km/h)')\n",
    "plt.ylabel('Traffic Volume')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Traffic Volume vs Congestion Level\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Congestion Level', y='Traffic Volume', data=df)\n",
    "plt.title('Traffic Volume vs Congestion Level')\n",
    "plt.xlabel('Congestion Level')\n",
    "plt.ylabel('Traffic Volume')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2105096",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Model Building\n",
    "### Traffic Volume Prediction - Random Forest Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df.drop(columns=['Traffic Volume', 'Congestion Level', 'Incident Reports'])\n",
    "y_traffic_volume = df['Traffic Volume']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_traffic_volume, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "(mse, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a998e7c",
   "metadata": {},
   "source": [
    "\n",
    "### Congestion Level Classification - Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556975e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Categorize congestion levels into Low, Medium, High\n",
    "congestion_labels = ['Low', 'Medium', 'High']\n",
    "df['Congestion Level Category'] = pd.cut(df['Congestion Level'], bins=[0, 33, 66, 100], labels=congestion_labels)\n",
    "\n",
    "# Separate features and target variable for classification\n",
    "y_congestion = df['Congestion Level Category']\n",
    "X_classification = df.drop(columns=['Congestion Level', 'Traffic Volume', 'Incident Reports', 'Congestion Level Category'])\n",
    "\n",
    "# Split the data for classification\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_classification, y_congestion, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_class = rf_classifier.predict(X_test_class)\n",
    "\n",
    "# Evaluate the model\n",
    "classification_rep = classification_report(y_test_class, y_pred_class, target_names=congestion_labels)\n",
    "\n",
    "classification_rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e09925",
   "metadata": {},
   "source": [
    "\n",
    "### Incident Detection - Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa647eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data for incident detection\n",
    "y_incident = df['Incident Reports']\n",
    "X_incident_classification = df.drop(columns=['Congestion Level', 'Traffic Volume', 'Incident Reports'])\n",
    "\n",
    "# Split the data for incident detection\n",
    "X_train_incident, X_test_incident, y_train_incident, y_test_incident = train_test_split(X_incident_classification, y_incident, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features for incident detection\n",
    "scaler = StandardScaler()\n",
    "X_train_incident_scaled = scaler.fit_transform(X_train_incident)\n",
    "X_test_incident_scaled = scaler.transform(X_test_incident)\n",
    "\n",
    "# Initialize Random Forest Classifier for incident detection\n",
    "rf_incident_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_incident_classifier.fit(X_train_incident_scaled, y_train_incident)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_incident = rf_incident_classifier.predict(X_test_incident_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "incident_classification_report = classification_report(y_test_incident, y_pred_incident)\n",
    "\n",
    "incident_classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5ac2a",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Traffic Flow Optimization - KMeans Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf37c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Select only the numeric columns for clustering\n",
    "X_clustering = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Normalize the features for clustering\n",
    "scaler_for_clustering = StandardScaler()\n",
    "X_clustering_scaled = scaler_for_clustering.fit_transform(X_clustering)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X_clustering_scaled)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Average Speed', y='Traffic Volume', hue='Cluster', palette='Set1', data=df, s=100, alpha=0.7)\n",
    "plt.title('Traffic Clusters Based on Traffic Volume and Average Speed')\n",
    "plt.xlabel('Average Speed (km/h)')\n",
    "plt.ylabel('Traffic Volume')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_centers_df = pd.DataFrame(cluster_centers, columns=X_clustering.columns)\n",
    "cluster_centers_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e91cf",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Model Evaluation and Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate models using metrics like RMSE, Accuracy, F1-Score, and MAE\n",
    "# Fine-tune models using GridSearchCV for better performance (to be added for optimization)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
